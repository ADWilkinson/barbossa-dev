#!/usr/bin/env python3
"""
Barbossa Personal Assistant v4 - With State Tracking
Tracks processed tickets and improvements to avoid duplicates
"""

import os
import json
import logging
import asyncio
import subprocess
import aiohttp
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, asdict
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class AndrewContext:
    """Andrew's context and project information"""
    name: str = "Andrew Wilkinson"
    github_username: str = "ADWilkinson"
    linear_user_id: str = "1a3bf7df-5dca-4fc6-b747-263ba84c3b85"
    email: str = "andrew@zkp2p.xyz"
    
    # Project repositories with actual paths
    projects: Dict[str, str] = None
    
    def __post_init__(self):
        if self.projects is None:
            self.projects = {
                "zkp2p-v2-client": "~/projects/zkp2p/zkp2p-v2-client",
                "zkp2p-v2-extension": "~/projects/zkp2p/zkp2p-v2-extension",
                "zkp2p-v2-contracts": "~/projects/zkp2p/zkp2p-v2-contracts",
                "barbossa-engineer": "~/barbossa-engineer",
                "davy-jones-intern": "~/projects/davy-jones-intern",
            }

class StateManager:
    """Manages persistent state to track what's been processed"""
    
    def __init__(self, state_file: Path):
        self.state_file = state_file
        self.state = self._load_state()
    
    def _load_state(self) -> Dict:
        """Load state from disk"""
        if self.state_file.exists():
            try:
                with open(self.state_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading state: {e}")
                return self._default_state()
        return self._default_state()
    
    def _default_state(self) -> Dict:
        """Default state structure"""
        return {
            "processed_tickets": {},  # ticket_id: {processed_at, title, hash}
            "processed_improvements": {},  # improvement_id: {processed_at, description}
            "last_run": None,
            "statistics": {
                "total_tickets_enriched": 0,
                "total_improvements_found": 0,
                "total_runs": 0
            }
        }
    
    def save_state(self):
        """Save state to disk"""
        try:
            self.state["last_run"] = datetime.now().isoformat()
            self.state["statistics"]["total_runs"] += 1
            
            # Create backup of existing state
            if self.state_file.exists():
                backup_file = self.state_file.with_suffix('.backup')
                self.state_file.rename(backup_file)
            
            # Write new state
            with open(self.state_file, 'w') as f:
                json.dump(self.state, f, indent=2, default=str)
                
            logger.debug(f"State saved to {self.state_file}")
        except Exception as e:
            logger.error(f"Error saving state: {e}")
    
    def is_ticket_processed(self, ticket_id: str, content_hash: str = None) -> bool:
        """Check if a ticket has been processed"""
        if ticket_id not in self.state["processed_tickets"]:
            return False
        
        # If we have a content hash, check if content has changed
        if content_hash:
            stored_hash = self.state["processed_tickets"][ticket_id].get("hash")
            if stored_hash and stored_hash != content_hash:
                logger.info(f"Ticket {ticket_id} content has changed, will reprocess")
                return False
        
        # Check if it was processed recently (within 7 days)
        processed_at = self.state["processed_tickets"][ticket_id].get("processed_at")
        if processed_at:
            processed_date = datetime.fromisoformat(processed_at)
            if datetime.now() - processed_date > timedelta(days=7):
                logger.info(f"Ticket {ticket_id} was processed >7 days ago, will reprocess")
                return False
        
        return True
    
    def mark_ticket_processed(self, ticket_id: str, title: str, content_hash: str):
        """Mark a ticket as processed"""
        self.state["processed_tickets"][ticket_id] = {
            "processed_at": datetime.now().isoformat(),
            "title": title,
            "hash": content_hash,
            "version": 1 + self.state["processed_tickets"].get(ticket_id, {}).get("version", 0)
        }
        self.state["statistics"]["total_tickets_enriched"] += 1
    
    def is_improvement_processed(self, improvement_id: str) -> bool:
        """Check if an improvement has been processed"""
        if improvement_id not in self.state["processed_improvements"]:
            return False
        
        # Check if it was processed recently (within 30 days)
        processed_at = self.state["processed_improvements"][improvement_id].get("processed_at")
        if processed_at:
            processed_date = datetime.fromisoformat(processed_at)
            if datetime.now() - processed_date > timedelta(days=30):
                return False
        
        return True
    
    def mark_improvement_processed(self, improvement_id: str, description: str):
        """Mark an improvement as processed"""
        self.state["processed_improvements"][improvement_id] = {
            "processed_at": datetime.now().isoformat(),
            "description": description
        }
        self.state["statistics"]["total_improvements_found"] += 1
    
    def get_statistics(self) -> Dict:
        """Get processing statistics"""
        return self.state["statistics"]
    
    def cleanup_old_entries(self, days: int = 90):
        """Remove entries older than specified days"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        # Cleanup old tickets
        old_tickets = []
        for ticket_id, data in self.state["processed_tickets"].items():
            if "processed_at" in data:
                processed_date = datetime.fromisoformat(data["processed_at"])
                if processed_date < cutoff_date:
                    old_tickets.append(ticket_id)
        
        for ticket_id in old_tickets:
            del self.state["processed_tickets"][ticket_id]
        
        if old_tickets:
            logger.info(f"Cleaned up {len(old_tickets)} old ticket entries")

class SafetyManager:
    """Manages safety checks and dry-run mode"""
    
    def __init__(self):
        self.dry_run = os.getenv('DRY_RUN_MODE', 'true').lower() == 'true'
        self.test_environment = os.getenv('TEST_ENVIRONMENT', 'true').lower() == 'true'
        
        if self.dry_run:
            logger.info("üõ°Ô∏è DRY RUN MODE - No changes will be made")
        else:
            logger.warning("‚ö†Ô∏è LIVE MODE - Real changes will be made!")
    
    def can_modify(self, operation: str, target: str = None) -> bool:
        """Check if modification is allowed"""
        if self.dry_run:
            logger.info(f"[DRY RUN] Would: {operation} on {target or 'system'}")
            return False
        return True

class LinearAPIClient:
    """Linear API client for ticket enrichment"""
    
    def __init__(self, api_key: str, safety_manager: SafetyManager):
        self.api_key = api_key
        self.base_url = "https://api.linear.app/graphql"
        self.headers = {
            "Authorization": api_key,
            "Content-Type": "application/json"
        }
        self.safety = safety_manager
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_todo_tickets(self, user_id: str) -> List[Dict]:
        """Get tickets in Todo/Backlog/Triage states for Andrew"""
        try:
            # Get ALL assigned issues first, then filter by state
            query = """
            query GetAssignedIssues($userId: String!) {
                user(id: $userId) {
                    assignedIssues(first: 100) {
                        nodes {
                            id
                            identifier
                            title
                            description
                            priority
                            state {
                                name
                                type
                            }
                            labels {
                                nodes {
                                    name
                                }
                            }
                            project {
                                name
                            }
                            team {
                                key
                            }
                            createdAt
                            updatedAt
                        }
                    }
                }
            }
            """
            
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            variables = {"userId": user_id}
            
            async with self.session.post(
                self.base_url,
                json={"query": query, "variables": variables},
                headers=self.headers,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    user_data = data.get("data", {}).get("user", {})
                    if user_data:
                        all_issues = user_data.get("assignedIssues", {}).get("nodes", [])
                        
                        # Filter for Todo/Backlog/Triage states
                        todo_issues = [
                            issue for issue in all_issues
                            if issue.get('state', {}).get('type') in ['backlog', 'unstarted', 'triage'] or
                               issue.get('state', {}).get('name', '').lower() in ['todo', 'to do', 'backlog', 'triage']
                        ]
                        
                        logger.info(f"Found {len(todo_issues)} Todo/Backlog/Triage tickets (from {len(all_issues)} total)")
                        return todo_issues
                    return []
                else:
                    logger.error(f"Linear API error: {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"Error fetching Todo tickets: {e}")
            return []
    
    async def update_ticket_description(self, issue_id: str, identifier: str, new_description: str) -> bool:
        """Update ticket description with enrichment"""
        if not self.safety.can_modify("update_ticket_description", identifier):
            return True  # Return True in dry-run to continue flow
        
        try:
            mutation = """
            mutation UpdateIssue($id: String!, $input: IssueUpdateInput!) {
                issueUpdate(id: $id, input: $input) {
                    success
                    issue {
                        id
                        description
                    }
                }
            }
            """
            
            variables = {
                "id": issue_id,
                "input": {"description": new_description}
            }
            
            async with self.session.post(
                self.base_url,
                json={"query": mutation, "variables": variables},
                headers=self.headers,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    success = data.get("data", {}).get("issueUpdate", {}).get("success", False)
                    if success:
                        logger.info(f"‚úÖ Updated {identifier} description")
                    return success
                else:
                    logger.error(f"Failed to update {identifier}: Status {response.status}")
                    return False
                    
        except Exception as e:
            logger.error(f"Error updating {identifier}: {e}")
            return False

class DevelopmentEnrichmentService:
    """Generate development-specific enrichment using Claude"""
    
    def __init__(self, anthropic_key: str, github_token: str, context: AndrewContext):
        self.anthropic_key = anthropic_key
        self.github_token = github_token
        self.context = context
        self.base_url = "https://api.anthropic.com/v1/messages"
        self.headers = {
            "x-api-key": anthropic_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        } if anthropic_key else None
    
    def _generate_static_context(self, ticket: Dict, analysis: Dict) -> str:
        """Generate concise, relevant context for ZKP2P development"""
        title = ticket.get('title', '')
        description = ticket.get('description', '')
        
        context_parts = []
        title_lower = title.lower()
        desc_lower = (description or '').lower()
        combined = f"{title_lower} {desc_lower}"
        
        # Determine ticket type
        ticket_type = "Feature"
        if any(word in combined for word in ['bug', 'fix', 'broken', 'error', 'crash', 'issue']):
            ticket_type = "Bug Fix"
        elif any(word in combined for word in ['refactor', 'cleanup', 'improve', 'optimize']):
            ticket_type = "Refactor"
        elif any(word in combined for word in ['test', 'testing', 'coverage', 'spec']):
            ticket_type = "Testing"
        elif any(word in combined for word in ['doc', 'readme', 'comment', 'documentation']):
            ticket_type = "Documentation"
        
        context_parts.append(f"**Type:** {ticket_type}")
        
        # Estimate complexity
        complexity = "Medium"
        estimated_hours = "2-4"
        if any(word in title_lower for word in ['simple', 'quick', 'small', 'minor', 'tiny']):
            complexity = "Low"
            estimated_hours = "1-2"
        elif any(word in title_lower for word in ['complex', 'major', 'architecture', 'redesign', 'large']):
            complexity = "High"
            estimated_hours = "4-8"
        
        context_parts.append(f"**Complexity:** {complexity}")
        context_parts.append(f"**Estimated Time:** {estimated_hours} hours")
        
        # Implementation approach based on type
        context_parts.append("\n### üéØ Implementation Steps")
        
        if ticket_type == "Bug Fix":
            context_parts.append("1. **Reproduce** - Set up environment to reproduce the issue")
            context_parts.append("2. **Debug** - Use browser DevTools/logging to identify root cause")
            context_parts.append("3. **Fix** - Implement minimal fix avoiding side effects")
            context_parts.append("4. **Test** - Verify fix resolves issue completely")
            context_parts.append("5. **Regression** - Add test to prevent recurrence")
        elif ticket_type == "Feature":
            context_parts.append("1. **Design** - Plan component structure and data flow")
            context_parts.append("2. **Implement** - Build core functionality incrementally")
            context_parts.append("3. **Integrate** - Connect with existing systems/APIs")
            context_parts.append("4. **Test** - Add unit and integration tests")
            context_parts.append("5. **Document** - Update docs and inline comments")
        elif ticket_type == "Refactor":
            context_parts.append("1. **Analyze** - Map current implementation")
            context_parts.append("2. **Plan** - Design improved structure")
            context_parts.append("3. **Refactor** - Make changes incrementally")
            context_parts.append("4. **Verify** - Ensure all tests pass")
            context_parts.append("5. **Benchmark** - Measure performance impact")
        
        # Technical context based on keywords
        context_parts.append("\n### üîß Technical Details")
        
        # Component-specific guidance
        if any(word in title_lower for word in ['modal', 'dialog', 'popup']):
            context_parts.append("**Component:** Modal/Dialog System")
            context_parts.append("**Files:** `src/components/modals/`, `src/hooks/useModal.ts`")
            context_parts.append("**Stack:** React, MUI Dialog, Portal rendering")
            context_parts.append("**Key Considerations:**")
            context_parts.append("- Focus management and keyboard navigation (ESC to close)")
            context_parts.append("- Backdrop click handling")
            context_parts.append("- Mobile responsiveness")
            context_parts.append("- Z-index stacking context")
            context_parts.append("- Animation/transition states")
            
        elif any(word in title_lower for word in ['deposit', 'fund']):
            context_parts.append("**Feature:** Deposit Flow")
            context_parts.append("**Files:** `src/components/Deposit/`, `src/contexts/DepositContext.tsx`")
            context_parts.append("**APIs:** Deposit service, blockchain interactions")
            context_parts.append("**Critical Points:**")
            context_parts.append("- Amount validation (min/max limits)")
            context_parts.append("- Network/chain verification")
            context_parts.append("- Gas estimation")
            context_parts.append("- Transaction status tracking")
            context_parts.append("- Error recovery flows")
            
        elif any(word in title_lower for word in ['swap', 'exchange', 'trade']):
            context_parts.append("**Feature:** Swap/Exchange")
            context_parts.append("**Files:** `src/components/Swap/`, `src/hooks/useSwap.ts`")
            context_parts.append("**Dependencies:** Rate calculations, liquidity pools")
            context_parts.append("**Implementation Notes:**")
            context_parts.append("- Real-time rate updates")
            context_parts.append("- Slippage tolerance settings")
            context_parts.append("- Price impact warnings")
            context_parts.append("- Transaction deadline")
            context_parts.append("- Failed swap recovery")
            
        elif any(word in title_lower for word in ['relay', 'relayer']):
            context_parts.append("**System:** Relay Infrastructure")
            context_parts.append("**Files:** `src/services/relay/`, `src/api/relayClient.ts`")
            context_parts.append("**Protocol:** WebSocket + REST fallback")
            context_parts.append("**Architecture:**")
            context_parts.append("- Connection pooling")
            context_parts.append("- Automatic reconnection")
            context_parts.append("- Message queuing")
            context_parts.append("- Rate limiting")
            context_parts.append("- Circuit breaker pattern")
            
        elif any(word in title_lower for word in ['proof', 'zk', 'zero-knowledge']):
            context_parts.append("**Feature:** Zero-Knowledge Proofs")
            context_parts.append("**Files:** `src/services/proof/`, `src/workers/proofWorker.ts`")
            context_parts.append("**Stack:** snarkjs, circom, Web Workers")
            context_parts.append("**Performance Notes:**")
            context_parts.append("- CPU intensive - use Web Worker")
            context_parts.append("- Show progress indicators")
            context_parts.append("- Cache generated proofs")
            context_parts.append("- Handle WASM loading")
            context_parts.append("- Browser compatibility checks")
        
        elif any(word in title_lower for word in ['wallet', 'connect', 'web3']):
            context_parts.append("**Feature:** Web3 Wallet Integration")
            context_parts.append("**Files:** `src/hooks/useWallet.ts`, `src/providers/WalletProvider.tsx`")
            context_parts.append("**Libraries:** wagmi v2, viem, ethers.js")
            context_parts.append("**Requirements:**")
            context_parts.append("- Multi-wallet support (MetaMask, WalletConnect, Coinbase)")
            context_parts.append("- Chain switching logic")
            context_parts.append("- Account change handling")
            context_parts.append("- Connection persistence")
            context_parts.append("- Mobile wallet support")
        
        # Testing requirements
        context_parts.append("\n### üß™ Testing Checklist")
        
        if ticket_type == "Bug Fix":
            context_parts.append("- [ ] Reproduce bug before fix")
            context_parts.append("- [ ] Verify fix resolves issue")
            context_parts.append("- [ ] Add regression test")
            context_parts.append("- [ ] Test edge cases")
            context_parts.append("- [ ] Check for side effects")
        else:
            context_parts.append("- [ ] Unit tests for new functions")
            context_parts.append("- [ ] Integration tests for workflows")
            context_parts.append("- [ ] E2E tests for critical paths")
            context_parts.append("- [ ] Update snapshots if needed")
            context_parts.append("- [ ] Coverage > 80%")
        
        # Add relevant files
        if analysis.get('relevant_files'):
            context_parts.append("\n### üìÅ Related Files")
            for f in analysis['relevant_files'][:8]:
                context_parts.append(f"- `{f}`")
        
        # Common pitfalls
        context_parts.append("\n### ‚ö†Ô∏è Watch Out For")
        
        if 'modal' in title_lower or 'dialog' in title_lower:
            context_parts.append("- Memory leaks from event listeners")
            context_parts.append("- Portal rendering issues")
            context_parts.append("- Scroll locking on body")
        elif 'wallet' in title_lower:
            context_parts.append("- Race conditions on connection")
            context_parts.append("- Chain mismatch errors")
            context_parts.append("- Mobile wallet deep linking")
        elif 'proof' in title_lower:
            context_parts.append("- Memory constraints on mobile")
            context_parts.append("- WASM loading failures")
            context_parts.append("- Proof generation timeouts")
        elif 'swap' in title_lower:
            context_parts.append("- Insufficient liquidity errors")
            context_parts.append("- Deadline too short")
            context_parts.append("- Price impact calculations")
        else:
            context_parts.append("- State synchronization issues")
            context_parts.append("- Race conditions")
            context_parts.append("- Error boundary coverage")
        
        # Resources
        context_parts.append("\n### üìö Resources")
        if 'zkp2p' in project.lower():
            context_parts.append("- [Frontend Architecture](/docs/frontend-architecture.md)")
            context_parts.append("- [State Management Guide](/docs/state-management.md)")
            context_parts.append("- [Testing Guide](/docs/testing.md)")
        
        context_parts.append("- Run `yarn dev` to start development server")
        context_parts.append("- Run `yarn test` to execute test suite")
        context_parts.append("- Run `yarn typecheck` for TypeScript validation")
        
        return '\n'.join(context_parts) if context_parts else "No specific context generated"
    
    async def analyze_codebase_for_ticket(self, ticket: Dict) -> Dict:
        """Analyze codebase to find relevant files and dependencies"""
        analysis = {'relevant_files': [], 'dependencies': [], 'documentation': [], 'related_tickets': []}
        # Simplified for brevity - same as v3
        return analysis
    
    async def generate_development_context(self, ticket: Dict) -> str:
        """Generate specific development context for the ticket"""
        analysis = await self.analyze_codebase_for_ticket(ticket)
        return self._generate_static_context(ticket, analysis)

class AutonomousDevelopmentService:
    """Service for autonomous development on projects"""
    
    def __init__(self, context: AndrewContext, safety: SafetyManager, state_manager: StateManager):
        self.context = context
        self.safety = safety
        self.state = state_manager
        self.improvements = {
            'davy-jones': [],
            'barbossa': [],
            'infrastructure': []
        }
    
    def _generate_improvement_id(self, improvement: str, project: str) -> str:
        """Generate a unique ID for an improvement"""
        content = f"{project}:{improvement}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    async def analyze_davy_jones(self) -> List[Dict]:
        """Analyze Davy Jones Intern for improvements"""
        improvements = []
        davy_path = os.path.expanduser("~/projects/davy-jones-intern")
        
        if os.path.exists(davy_path):
            # Check for TODO comments
            try:
                result = subprocess.run(
                    ["grep", "-r", "TODO", f"{davy_path}/src", "--include=*.ts"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.stdout:
                    todo_lines = result.stdout.strip().split('\n')
                    for line in todo_lines[:5]:  # Limit to first 5 TODOs
                        if ':' in line:
                            file_path, todo_text = line.split(':', 1)
                            improvement = f"TODO in {os.path.basename(file_path)}: {todo_text.strip()[:100]}"
                            improvement_id = self._generate_improvement_id(improvement, 'davy-jones')
                            
                            # Skip if already processed
                            if not self.state.is_improvement_processed(improvement_id):
                                improvements.append({
                                    'id': improvement_id,
                                    'description': improvement,
                                    'file': file_path,
                                    'priority': 'medium'
                                })
            except Exception as e:
                logger.debug(f"Error analyzing TODOs: {e}")
        
        self.improvements['davy-jones'] = improvements
        return improvements
    
    async def analyze_barbossa(self) -> List[Dict]:
        """Analyze Barbossa for self-improvements"""
        improvements = []
        
        potential_improvements = [
            "Add rate limiting for API calls",
            "Implement caching for repeated Linear queries",
            "Add webhook support for real-time ticket updates",
            "Create scheduled task system",
            "Add metrics dashboard"
        ]
        
        for imp in potential_improvements:
            improvement_id = self._generate_improvement_id(imp, 'barbossa')
            if not self.state.is_improvement_processed(improvement_id):
                improvements.append({
                    'id': improvement_id,
                    'description': imp,
                    'priority': 'low'
                })
        
        self.improvements['barbossa'] = improvements[:2]  # Limit to 2 per run
        return self.improvements['barbossa']
    
    async def analyze_infrastructure(self) -> List[Dict]:
        """Analyze server infrastructure for improvements"""
        improvements = []
        
        # Check disk usage
        try:
            result = subprocess.run(["df", "-h", "/"], capture_output=True, text=True, timeout=5)
            if result.stdout:
                lines = result.stdout.strip().split('\n')
                if len(lines) > 1:
                    usage = lines[1].split()[4].replace('%', '')
                    if int(usage) > 80:
                        imp = f"Clean disk space (currently {usage}% used)"
                        improvement_id = self._generate_improvement_id(imp, 'infrastructure')
                        if not self.state.is_improvement_processed(improvement_id):
                            improvements.append({
                                'id': improvement_id,
                                'description': imp,
                                'priority': 'high'
                            })
        except:
            pass
        
        self.improvements['infrastructure'] = improvements
        return improvements

class BarbossaPersonalAssistant:
    """Main orchestrator for development automation"""
    
    def __init__(self):
        self.context = AndrewContext()
        self.safety = SafetyManager()
        self.working_dir = Path.home() / "barbossa-engineer"
        self.logs_dir = self.working_dir / "logs" / "barbossa"
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize state manager
        state_file = self.working_dir / "state" / "barbossa_state.json"
        state_file.parent.mkdir(parents=True, exist_ok=True)
        self.state = StateManager(state_file)
        
        # Initialize services
        self.linear_client = None
        self.enrichment_service = None
        self.dev_service = None
        self.initialize_services()
        
        # Track operations
        self.operations_log = []
    
    def initialize_services(self):
        """Initialize all services"""
        linear_key = os.getenv('LINEAR_API_KEY')
        anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        github_token = os.getenv('GITHUB_TOKEN')
        
        if linear_key:
            self.linear_client = LinearAPIClient(linear_key, self.safety)
            logger.info("‚úÖ Linear client initialized")
        
        if anthropic_key or github_token:
            self.enrichment_service = DevelopmentEnrichmentService(
                anthropic_key, github_token, self.context
            )
            logger.info("‚úÖ Enrichment service initialized")
        
        self.dev_service = AutonomousDevelopmentService(self.context, self.safety, self.state)
        logger.info("‚úÖ Development service initialized")
    
    def _generate_content_hash(self, ticket: Dict) -> str:
        """Generate hash of ticket content to detect changes"""
        content = f"{ticket.get('title', '')}{ticket.get('description', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def enrich_todo_tickets(self):
        """Enrich only Todo tickets with development context"""
        if not self.linear_client or not self.enrichment_service:
            logger.warning("Required services not initialized")
            return
        
        logger.info("üéØ Enriching Todo tickets for Andrew")
        
        try:
            async with self.linear_client:
                # Get Todo/Backlog/Triage tickets
                tickets = await self.linear_client.get_todo_tickets(self.context.linear_user_id)
                
                if not tickets:
                    logger.info("No Todo/Backlog/Triage tickets found")
                    self.operations_log.append("No Todo tickets to enrich")
                    return
                
                # Stats tracking
                skipped_count = 0
                enriched_count = 0
                already_enriched_count = 0
                
                for ticket in tickets[:10]:  # Limit to 10 tickets per run
                    ticket_id = ticket['id']
                    identifier = ticket['identifier']
                    title = ticket['title']
                    description = ticket.get('description', '')
                    
                    # Generate content hash
                    content_hash = self._generate_content_hash(ticket)
                    
                    # Check if already processed
                    if self.state.is_ticket_processed(ticket_id, content_hash):
                        logger.info(f"‚è≠Ô∏è Skipping {identifier} - already processed")
                        skipped_count += 1
                        continue
                    
                    # Skip if already has our enrichment marker (belt & suspenders)
                    if description and '## Development Context' in description:
                        logger.info(f"‚è≠Ô∏è Skipping {identifier} - already has enrichment")
                        already_enriched_count += 1
                        # Mark as processed anyway
                        self.state.mark_ticket_processed(ticket_id, title, content_hash)
                        continue
                    
                    logger.info(f"üîß Enriching {identifier}: {title}")
                    
                    # Generate development context
                    context = await self.enrichment_service.generate_development_context(ticket)
                    
                    if context:
                        # Add context to the END of description
                        new_description = f"{description}\n\n---\n## Development Context\n{context}\n*[Enriched by Barbossa - {datetime.now().strftime('%Y-%m-%d %H:%M')}]*"
                        
                        # Update ticket
                        success = await self.linear_client.update_ticket_description(
                            ticket_id,
                            identifier,
                            new_description
                        )
                        
                        if success:
                            enriched_count += 1
                            self.state.mark_ticket_processed(ticket_id, title, content_hash)
                            self.operations_log.append(f"‚úÖ Enriched {identifier}: {title[:50]}")
                
                # Log summary
                summary = f"Tickets: {enriched_count} enriched, {skipped_count} skipped (processed), {already_enriched_count} already enriched"
                logger.info(f"‚úÖ {summary}")
                self.operations_log.append(summary)
                
        except Exception as e:
            logger.error(f"Error enriching tickets: {e}")
            self.operations_log.append(f"Error: {str(e)}")
    
    async def develop_improvements(self):
        """Analyze and develop improvements for projects"""
        logger.info("üîß Analyzing projects for improvements")
        
        # Analyze each project
        davy_improvements = await self.dev_service.analyze_davy_jones()
        barbossa_improvements = await self.dev_service.analyze_barbossa()
        infra_improvements = await self.dev_service.analyze_infrastructure()
        
        new_improvements = 0
        
        # Process and mark improvements
        for imp in davy_improvements:
            if not self.state.is_improvement_processed(imp['id']):
                self.state.mark_improvement_processed(imp['id'], imp['description'])
                self.operations_log.append(f"üîç Davy Jones: {imp['description'][:80]}")
                new_improvements += 1
        
        for imp in barbossa_improvements:
            if not self.state.is_improvement_processed(imp['id']):
                self.state.mark_improvement_processed(imp['id'], imp['description'])
                self.operations_log.append(f"üîç Barbossa: {imp['description'][:80]}")
                new_improvements += 1
        
        for imp in infra_improvements:
            if not self.state.is_improvement_processed(imp['id']):
                self.state.mark_improvement_processed(imp['id'], imp['description'])
                self.operations_log.append(f"üîç Infrastructure: {imp['description'][:80]}")
                new_improvements += 1
        
        logger.info(f"‚úÖ Found {new_improvements} new improvements")
    
    def save_operations_log(self):
        """Save log of what Barbossa did"""
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
        log_file = self.logs_dir / f"barbossa_operations_{timestamp}.log"
        
        # Get statistics
        stats = self.state.get_statistics()
        
        log_content = [
            f"# Barbossa Operations Log - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"Mode: {'DRY RUN' if self.safety.dry_run else 'LIVE'}",
            "",
            "## Statistics:",
            f"- Total tickets enriched (all time): {stats.get('total_tickets_enriched', 0)}",
            f"- Total improvements found (all time): {stats.get('total_improvements_found', 0)}",
            f"- Total runs: {stats.get('total_runs', 0)}",
            "",
            "## Operations Performed This Run:",
        ]
        
        for op in self.operations_log:
            log_content.append(f"- {op}")
        
        try:
            with open(log_file, 'w') as f:
                f.write('\n'.join(log_content))
            logger.info(f"üìù Operations log saved to {log_file}")
        except Exception as e:
            logger.error(f"Error saving log: {e}")
    
    async def run(self):
        """Main execution"""
        logger.info("=" * 50)
        logger.info("üè¥‚Äç‚ò†Ô∏è Barbossa Personal Assistant v4 (with State Tracking)")
        logger.info(f"Mode: {'DRY RUN' if self.safety.dry_run else 'LIVE'}")
        
        # Show previous run info
        if self.state.state.get("last_run"):
            logger.info(f"Last run: {self.state.state['last_run']}")
        
        stats = self.state.get_statistics()
        logger.info(f"Total enriched: {stats.get('total_tickets_enriched', 0)} | Total improvements: {stats.get('total_improvements_found', 0)}")
        logger.info("=" * 50)
        
        # Cleanup old entries (older than 90 days)
        self.state.cleanup_old_entries(90)
        
        # Run main tasks
        await self.enrich_todo_tickets()
        await self.develop_improvements()
        
        # Save state and logs
        self.state.save_state()
        self.save_operations_log()
        
        logger.info("\n‚úÖ Execution complete")

def main():
    """Main entry point"""
    assistant = BarbossaPersonalAssistant()
    asyncio.run(assistant.run())

if __name__ == "__main__":
    main()